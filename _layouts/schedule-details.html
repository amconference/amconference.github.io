{% include header.html %}
{% include nav.html %}

<h2>Session details</h2>

<div id="sessions-details">
  <h3>Wednesday 7th October 2015</h3>

  <hr>

  <h4>9:00 - 9:30:<br>Registration</h4>

  <hr>

  <h4>9:30 - 9:45:<br>Welcome to the conference</h4>

  <hr>

  <h4 id="standards">9:45 - 11:00:<br>Standards in altmetrics</h4>

  <p><strong>Chair:</strong> Fiona Murphy, Senior Associate at Maverick Publishing Specialists</p>

  <ul>
  <li><strong>Geoff Bilder, CrossRef</strong></li>
  <li>
  <p><strong>Zohreh Zahedi, CWTS-Leiden University</strong></p>

  <ul>
  <li>
  <p><em>Challenges in altmetric data collection: are there differences among different altmetric providers/aggregators?</em></p>

  <p>This project focuses on the study of data collection consistency across three altmetrics providers or aggregators: Altmetric.com, Mendeley and the Open Source software Lagotto (used by PLOS, CrossRef and others). The aim is to explore if metrics for a same set of publications are consistent across them. A random sample of 30,000 Crossref (15,000) and WoS (15,000) DOIs from 2013 has been considered. The data collection has been done at the same date/time on July 23 2015 starting at 2 PM CEST using the Mendeley REST API, Altmetric.com dump file and the Lagotto open source application. Several discrepancies among these altmetrics data providers in reporting metrics have been found. Regarding the coverage of DOIs per provider, Mendeley has the highest coverage 20,677 (69%), Lagotto 20,364 (68%) and altmetric.com 6,946 (21%). As expected Mendeley provides the highest values of readership counts compared to Lagotto and altmetric.com. Lagotto provides the highest number of Facebook counts, Reddit mentions and CiteULike counts. Altmetric.com provides the highest number of tweets. Regarding ‘intensity’ (average counts for the papers with at least one event) there are differences across the data providers in the common data sources (Tweets, Facebook, CiteULike, Reddit and Mendeley counts). Regarding overlapping papers with metrics, Altmetric.com has a higher twitter coverage (21%) and Facebook coverage (5%) than Lagotto Twitter (0.1%) and Lagotto Facebook (4%). For CiteULike (2.5%) and Reddit (36%) Lagotto has higher coverage than altmetric.com of CiteULike (1.9%) and Reddit (26%). There are some differences for Mendeley readerships as well and for a small set of DOIs with higher Mendeley reader counts reported by Lagotto and altmetric.com than Mendeley itself.</p>

  <p>The reasons for the different metrics relate to the different methods in collecting and processing metrics by the different providers. How each provider queries from sources also matters (using DOI or other metadata), using different APIs (for example for Facebook and Twitter) or possible time lags in the data collection or updating issues. Furthermore, if the data provider is reporting the public Facebook counts or public tweets or compiling all the retweets or favorites in one metric or as a separate value cause differences in the counts. There are also issues with tracking DOIs from difference registration agencies Moreover, there are issues with the quality of metadata for which altmetrics are collected, for example differences in publication dates between WoS and Crossref. Other problems include accessibility issues (e.g. with Twitter), resolving DOIs to URLs issues (e.g. differences across publisher platforms in resolving DOIs to journal landing pages), etc. These results emphasize the need for adhering to best practices in altmetric data collection both by altmetric providers and the publishers. Future steps include developing guidelines and recommendations regarding altmetric data collection to introduce transparency and consistent across providers. NISO in 2015 has initiated a working group on altmetrics data quality and the group has developed a draft code of conduct for collection, processing, dissemination and reuse of altmetric data.</p>
  </li>
  </ul>
  </li>
  </ul>

  <hr>

  <h4>11:00 - 11:20:<br>Coffee break</h4>

  <hr>

  <h4 id="in-the-library">11:20 - 12:35:<br>Altmetrics in the library</h4>

  <p><strong>Chair:</strong> Martijn Roelaandse, Manager Publishing Innovation at Springer Science+Business Media</p>

  <ul>
  <li>
  <p><strong>Stacy Konkiel (Altmetric.com) &amp; Sarah Sutton (Emporia State University)</strong></p>

  <ul>
  <li>
  <p><em>Using altmetrics for collection development and other duties in US academic libraries</em></p>

  <p>As the awareness of altmetrics continues to grow among librarians, some have posited that altmetrics may be useful for library collection development and other day-to-day tasks. In this talk, the presenters will share the preliminary results from a survey of academic librarians in the United States on their practical uses of altmetrics data. We'll also discuss awareness of altmetrics in general among collection development librarians as compared to other groups of librarians, and the implications of our results for both libraries and the field of altmetrics in general. With this session, we aim to foster a frank discussion among attendees--including bibliometricians, altmetrics app developers, and many non-US librarians--about the realities of altmetrics uptake in academic libraries in the the United States, and how our assumptions can differ from reality.</p>
  </li>
  </ul>
  </li>
  <li><p><strong>Kristi Holmes</strong></p></li>
  <li><strong>Wouter Gerritsma, VU in Amsterdam</strong></li>
  <li>
  <p><strong>Alenka Princic, TU Delft Library, Netherlands</strong></p>

  <ul>
  <li>
  <p><em>Alternative metrics at Dutch university libraries</em></p>

  <p>The role of a librarian at many universities and research institutions has changed and is still changing. For most libraries a common thread is the knowledge flow and technological innovations in order to support the researchers in augmenting their academic output, academic visibility and impact. A challenge for the Library however, is how to contribute to the increased visibility of the researchers beyond the obvious publication strategy and how to make it measurable. Alternative metrics makes part of these endeavours. When it comes to altmetrics an obvious role of a library is analysis of the market, comparison of the different tools and advice to the end users; to a lesser extent the implementation of altmetrics at an institutional level. Similarly as several Dutch libraries, the TU Delft Library is currently conducting a comparative analysis of the major tools on the market. The Delft library, however, will make a step further by experimenting with the use of different tools on a pilot scale in order to be able to implement alternative metrics at an institutional level and embed it in the research lifecycle. These plans will be presented at the conference. The situation in the rest of the Netherlands may be somewhat different. Other universities in the Netherlands may approach alternative metrics in a different way and have different visions. Up to now no concrete initiatives have been taken to approach alternative metrics centrally - on a national level. TU Delft is conducting a survey to collect the data from the Dutch university libraries on their approach to altmetrics. In the questionnaire the university libraries are asked about their vision with regard to altmetrics, their goals, barriers, implementation plans, and if already set in place - about their workflows and usage statistics. The results of this survey will be presented as well as some thoughts on future steps at the Dutch university libraries.</p>
  </li>
  </ul>
  </li>
  </ul>

  <hr>

  <h4>12:35 - 13:35:<br>Lunch (&amp; Posters)</h4>

  <hr>

  <h4 id="good-bad-pointless">13:35 - 14:20:<br>Science outreach - the good, the bad, and the pointless (keynote)</h4>

  <p><strong>Chair:</strong> Ian Mulvany, Head of technology for eLife</p>

  <ul>
  <li>
  <p><strong>Simon Singh</strong></p>

  <ul>
  <li>
  <p><em>Science outreach - the good, the bad, and the pointless</em></p>

  <p>Simon Singh will be presenting the keynote talk at this year's conference. Simon is a science writer and former TV producer, who joined the BBC in 1991. Before becoming a science journalist, he completed a PhD in particle physics at the University of Cambridge. His talk, Science Outreach - the good, the bad and the pointless will look at the range of science outreach activities, and ask what sort of activities might have an impact, and which are a complete waste of time, money and effort.</p>
  </li>
  </ul>
  </li>
  </ul>

  <hr>

  <h4 id="good-bad-pointless-panel">14:20 – 15:05:<br>Science outreach - the good, the bad, and the pointless (panel discussion)</h4>

  <p><strong>Chair:</strong> Ian Mulvany, Head of technology for eLife</p>

  <ul>
  <li>
  <p><strong>Simon Singh</strong></p>

  <p>Simon Singh is a science writer and former TV producer, who joined the BBC in 1991. Before becoming a science journalist, he completed a PhD in particle physics at the University of Cambridge.</p>
  </li>
  <li>
  <p><strong>Neuroskeptic</strong></p>

  <p>Neuroskeptic is a pseudonymous blogger for Discover Magazine and PLOS Neuroscience. He/she/they write about neuroscience, psychology and psychiatry through a skeptical lens.</p>
  </li>
  <li><p><strong>Lucy van Hilten</strong></p></li>
  </ul>

  <hr>

  <h4>15:05 - 15:25:<br>Coffee break</h4>

  <hr>

  <h4 id="indicators">15:25 - 16:40:<br>Altmetrics as indicators of economic and social impact</h4>

  <p><strong>Chair:</strong> Euan Adie, Founder of Altmetric.com</p>

  <ul>
  <li>
  <p><strong>Anup Kumar Das, Jawaharlal Nehru University</strong></p>

  <ul>
  <li>
  <p><em>Altmetrics and the Changing Societal Needs of Research Communications at R&amp;D Centres in an Emerging Country: A Case Study of India</em></p>

  <p>Altmetrics necessitates changing research communications strategies in research departments in universities and R&amp;D centres in emerging countries. However, there is a lack of specialization or professionalization in managing and formulating research communications strategies in these research centres. This makes these R&amp;D centres less compatible to global outreach. Research papers emanated from these R&amp;D centres and departments don’t get adequate international visibility or attention or recognition due to lack of planning and strategic approaches by the policymakers and institutional decision makers. Research communications should also embrace the specialization available in the research department. For example, the services of documentation officers, research officers and information scientists available at the research departments in Indian universities can be adequately utilized for strategic planning and increasing institutional outreach to global scholarly audiences. They can be trained and made custodian of the research communications channels freely available online such as academic social networks, institutional repositories, open data repositories, and electronic discussion forums. There is also need of creating a rank of specialized personnel for managing institutional research communications channels. However, in many developing countries, including in India, the R&amp;D centres and research departments are in public sector and they need to take proactive role. Thus, every R&amp;D centre and research department should think of engaging the information specialists or re-designating their research librarians for managing their research communications channels. This paper will highlight a model institutional research communications strategy to increase global visibility of the institutional research. This paper will also cite a case where strategic use of research communications channels in an Indian institution bears the fruits of higher altmetric scores and global visibility of institutional researchers.</p>
  </li>
  </ul>
  </li>
  <li>
  <p><strong>Pablo Alperin, Simon Fraser University</strong></p>

  <ul>
  <li>
  <p><em>Evolving altmetrics to capture impact outside the academy</em></p>

  <p>There are an infinite number of ways in which a research article can serve society without ever being mentioned on the Web. The most often cited example is that of a patient walking into a doctor's office with information about a new medical treatment in the hopes of better managing their illness. There are, however, more subtle ways in which the access to research can have an impact on society. A similar patient may read about a treatment and simply be given hope about their prognosis. Other examples might include an NGO tweaking their programs to better serve their community as a result of reading a study, or a government agency taking up a policy recommendation from a paper. Within academia, there are also societal impacts that cannot be measured by citations. A paper used for didactic purposes in a classroom, even if never cited in a student essay, contributes to the human capital development, and as a result, to the strengthening of the higher education system as a whole. These are but a few uses that will never be captured by citations, but that could potentially be glean from altmetrics (or, more likely, from a future iteration of what we now know as altmetrics).</p>

  <p>Looking at the number of times scholarly articles are mentioned on the Web will never be an adequate way of understanding these indirect and subtle forms of the societal impact of research. While we have not yet reached the limits of what altmetrics can offer us in their current form, we can be sure that altmetrics today will do not yield much insight into the societal impact of research. To explore the societal impact of research through altmetrics, they must evolve to better capture: 1) who are the people that generate events that lead to metrics; and 2) what are the contexts and rationales that trigger those events. It is only by exploring these questions that practitioners and researchers will be able to guide the development of altmetrics to capture the type of non-academic impacts that many hope they will.</p>

  <p>In a small-scale study attempting to answer the first question, I found that Twitter users who shared articles from the Brazilian collection of the Scientific Electronic Library Online (SciELO) were made up of a high percentage of non-academics, including professionals, homemakers, patient groups, independent researchers, and journalists. The extent to which these users are present among those who share research from other regions, languages, and disciplines is yet to be studied, but if it is similarly widespread, then Twitter may prove to be a suitable place to search for impact of research outside of the academy. It is therefore necessary to evolve altmetrics to better identify these groups and the ways in which they use the research.</p>
  </li>
  </ul>
  </li>
  <li>
  <p><strong>Ashby (SAGE) &amp; Mathias Astell (Nature Publishing Group)</strong></p>

  <ul>
  <li>
  <p><em>The Empty Chair at the Metrics Table: Discussing the absence of educational impact metrics, and a framework for their creation</em></p>

  <p>The recent drive to establish more nuanced metrics to account for the different forms of a scholarly output’s impact - so called altmetrics - has largely neglected one key element of the scholarly research cycle: educational impact. Recent shifts in the assessment of UK university outputs, including a mooted Teaching Excellence Framework to accompany the next REF cycle, suggest that the work achieved by scholars in education (i.e., the impact on student learning) is becoming a more central part of performance assessment. Such a shift may go some way towards reducing the current unhelpful bifurcation of academia into research and education – however, without a set of metrics by which to complement any such assessments, education and teaching will ultimately remain on an unequal footing with research. It is telling in this respect that the recent 163 page report on the use of metrics in the scholarly world, The Metric Tide, only made 5 mentions of educational impact.</p>

  <p>To begin to unpack how best scholarly outputs can be assessed for educational impact, the following presentation will consider the ways in which scholarly outputs - for the purposes of this presentation defined as any DOI trackable output in any academic field - play a role within the areas of teaching excellence and overall learning at Higher Education Institutes (HEIs). The presentation will then move on to explore the different educational impact pathways of scholarly outputs within these areas, along with the sources of data (such as online syllabi, online university and college course reading lists, graduate dissertations and theses, educational presentations, digital journal clubs, reference management system data, university and public library holdings and usage data, as well as digital library holdings and usage data) and analysis techniques (similar to those utilised in the assessment of other alternative metrics, e.g., frequency of appearance, usage, citations, etc.) that could be utilised to create the basis for a robust set of educational impact metrics.</p>

  <p>In exploring the use of scholarly outputs in HEIs and the educational impact pathways these outputs follow, along with an assessment of the data available through which to track these elements, this presentation will lay down the basic tenets of a new paradigm in academic metrics: one tracking educational impact of scholarly outputs.</p>
  </li>
  </ul>
  </li>
  <li>
  <p><strong>Prof Theng Yin Leng, Nanyang Technological University, Singapore</strong></p>

  <ul>
  <li>
  <p><em>Altmetrics: Rethinking and Exploring New Ways of Measuring Research Outputs</em></p>

  <p>Traditional metrics for scholarly and scientific publishing such as citation counts and impact factors have been criticised for being too shallow and too narrow. In order to go beyond limited comparisons of like-for-like and to become generally useful, we need better metrics, techniques and tools to measure research outputs more accurately.</p>

  <p>Worldwide, universities, research institutes and policy makers realise that there is a need to have metrics that allow equitable comparisons, and can be used to benchmark research outputs, and efforts are needed to develop more robust metrics for general use. Today, with access to large volumes of data from diverse sources, it is possible to define alternative research impact metrics that capture domain-specific insights.</p>

  <p>Therefore, the overall aim of this project is to investigate new approaches offered by Interactive Digital Technology and Social Media to rethink and explore ways to measure research outputs. Specifically, this project focuses on scholarly and scientific publishing in which in recent years, altmetrics have appeared as new and alternative metrics. We intend to concentrate on exploring and rethinking ways of measuring research outputs of a scholarly and scientific nature of individuals and aggregated individuals belonging to a university, institute or centre. We aim to design, develop and evaluate a system prototype called “Altmetrics for Research Impact Actuation” (ARIA), to gather and compute altmetrics to measure research impact.</p>

  <p>The specific objectives of the project are:</p>

  <p>a) Conduct a literature survey of traditional metrics and altmetrics to measure research outputs.<br>
  b) Develop a framework and a set of hypotheses for cross-metric validation.<br>
  c) Develop new metrics on research impact that can be harvested from social media platforms (e.g. Twitter, Weibo, etc.).<br>
  d) Develop new research impact metrics for the different disciplines, broadly divided into: (i) “hard sciences” research which include sciences, computing and engineering; (ii) “non-hard sciences” research humanities, arts, business and social sciences.<br>
  e) Develop new metrics to measure research innovation and commercialisation.<br>
  f) Design, develop and evaluate the ARIA prototype based on inputs from (a) to (e).</p>

  <p>This project started in February 2015 and will progress over three years. We plan to evaluate our approach with researchers from the Nanyang Technological University (NTU) in Singapore, cooperating with the NTU library and the NTU research office. This research will help funding agencies, and educational and research institutions, to measure and benchmark research outputs. We believe that findings from this project will be useful in our long-term objectives of computing better measurements of research outputs, hence raising standards.</p>
  </li>
  </ul>
  </li>
  <li>
  <p><strong>Costas, CWTS-Leiden University</strong></p>

  <ul>
  <li>
  <p><em>Citation theories and their application to altmetrics</em></p>

  <p>Researchers are increasingly pressured to provide evidence that their work has impact—within the scientific community, on the economy or on society at large. Altmetrics have been suggested as indicators of various forms of impact aiming to replace citations as the sole measure of academic success. The acts on which various altmetrics are based are, however, quite heterogeneous: likes on Facebook, mentions on Twitter, saves on Mendeley, and expert recommendations on F1000 are acts that differ in terms of user community, engagement, motivation and audience. While the act of citing has been an essential part of the scholarly communication process since the beginning of modern science, it is unclear whether the acts that altmetrics are based on are relevant in scholarly communication. While empirical studies have shown that most altmetrics correlate weakly with citations—suggesting fundamental differences between these various metrics—conceptual discussions about their meaning are rare, leaving it unclear what is actually measured. We argue that the heterogeneity of user communities, levels of engagement, motivations, as well as of the audiences to which the various metrics are associated affects their respective meaning.</p>

  <p>In order to go beyond correlation analyses and better understand the meaning of altmetrics, we discuss acts on social media from a theoretical and conceptual perspective using citation theories, chosen because of the analogy that is often made between altmetrics and citations. By discussing various acts on social media from the perspective of the normative, social constructivist, and concept symbols citation theories (Haustein, Bowman, &amp; Costas, 2015), we show that they are more or less suitable, and that the level to which they can apply depends on the particular act considered. For example, Merton’s ethos of science can help to explain the mechanisms behind reviewing and recommending on F1000 and (to some extent) blog citations and Mendeley readership. Users on Twitter and Facebook are less likely to adhere to these norms, as can be reflected by the important tweeting intensity associated with papers that have funny titles, focus on curious topics or to retracted publications. Social constructivist theories, and in particular the Matthew effect, seem applicable to most acts on social media due to their networked nature; explaining the concentration of events on a few prominent papers or actors on social media. The concept symbols theory was helpful to explain mentions on Twitter, for example with respect to hashtags.</p>

  <p>Theoretical discussions will be supported by concrete practical examples to begin debates surrounding the development of a framework that accommodates the broad and diverse spectrum of acts recorded by different altmetric sources.</p>
  </li>
  </ul>
  </li>
  </ul>

  <hr>

  <h4 id="workshops">16:30 - 17:40:<br>Workshop Sessions</h4>

  <p><strong>Library chair:</strong> Jennifer Lin, CrossRef<br><br>
  <strong>Economic and social impact chair:</strong> Ian Mulvany, eLife</p>

  <hr>

  <h4>17:40 - 19:00:<br>Drinks reception</h4>

  <hr>

  <h3>Thursday 8th October 2015</h3>

  <hr>

  <h4>9:30 - 9:45:<br>Introduction</h4>

  <hr>

  <h4 id="altmetrics-now">9:45 - 11:00:<br>How are people using altmetrics now?</h4>

  <p><strong>Chair:</strong> Catherine Williams, Altmetric.com</p>

  <ul>
  <li>
  <p><strong>Andrea Michalek, Plum Analytics</strong></p>

  <ul>
  <li>
  <p><em>10 in 10: 10 Altmetrics Use Cases in 10 Minutes</em></p>

  <p>A wide variety of institutions are adopting comprehensive metrics to uncover and tell the stories of their research. These include academic libraries, grant funders, publishers and more. This session will showcase ten different use cases of altmetrics from 10 different users. We will see everything from metrics in an institutional repository to metrics in a grant search system. We will hear stories about how altmetrics helped an early career researcher and more.</p>
  </li>
  </ul>
  </li>
  <li>
  <p><strong>Andrew Beeken, University of Lincoln</strong></p>

  <ul>
  <li>
  <p><em>The University of Lincoln Research Bridge</em></p>

  <p>Measuring the reach and impact of research publications is important in helping UK HEIs respond to challenges in the research landscape. With the modern digital publishing ecosystem, the measurement of emerging alternative metrics is becoming increasingly key, as well as identifying how they can be represented and interpreted across multiple disciplines, sometimes using abstract, synthesized values. This can often be a difficult task to do without specific, specialised tools.</p>

  <p>To help bring all of these distinct types of measures together, the University of Lincoln is working on a prototype “Research Bridge” platform for use across the institution. This will assist in helping to make sense of not only the currently available metrics, but also institution specific synthesized metrics while being flexible enough to include emerging measures as they become relevant.</p>

  <p>This presentation will look at the issues surrounding gathering information from disparate systems, the benefits of an open culture and how using altmetrics alongside other available measurements can change the academic perspective on research impact.</p>
  </li>
  </ul>
  </li>
  <li>
  <p><strong>Chantelle Rijs, Frontiers</strong></p>

  <ul>
  <li>
  <p><em>Linking published articles to a research network - a new perspective on the use of altmetrics.</em></p>

  <p>We’re accustomed to aggregating and analyzing impact data at an article level. We make correlations between and draw conclusions from the types of impact different article types in different specialties are having across our incredibly diverse scholarly ecosystem. Academic profile information allows for a range of new, enriched associations to be made between the demographic information of the authors and the impact, including altmetrics, of their published work.</p>

  <p>The Frontiers publishing platform is fully integrated into Loop, the new research network for academics, both of which have integrated Altmetric. These enhanced connections provide a multitude of interesting insights.</p>

  <p>This presentation will examine the unique linking between published articles, research networks and Altmetric and how the bridging of these gaps highlights new ways to apply altmetrics, including into customized marketing campaigns and article tiering systems at a publishing level and at a research network level: author impact updates and improved content discovery.</p>
  </li>
  </ul>
  </li>
  <li>
  <p><strong>Tuija Sonkkila, Aalto University, Leadership Support Services</strong></p>

  <ul>
  <li>
  <p><em>CRIS and altmetrics</em></p>

  <p>Current Research Information Systems (CRIS) are among the youngest members in the family of support services at academic institutions. However, CRIS systems may be slow in adapting new features. For instance, incorporating various altmetrics could be too much to ask. One obvious answer to this demand is to consider the commercial altmetrics products. Something can be done locally, too.</p>

  <p>Aalto University CRIS will be launched in 2016. In this talk, I will present some findings on how data from the Pure CRIS system could be used to build a lightweight web application by joining them with data from free altmetrics APIs.</p>
  </li>
  </ul>
  </li>
  </ul>

  <hr>

  <h4>11:00 - 11:20:<br>Coffee break</h4>

  <hr>

  <h4 id="open-science">11:20 - 12:05:<br>Altmetrics and open science</h4>

  <p><strong>Chair:</strong> Mike Taylor, Elsevier Labs</p>

  <ul>
  <li>
  <p><strong>William Gunn, Mendeley</strong></p>

  <ul>
  <li>
  <p><em>Understanding reader intent: Who are the readers of a papers on Mendeley and why do they add what they add to their libraries?</em></p>

  <p>With an open catalog of millions of papers and applications used by millions of researchers to organize, share, and discover research, Mendeley is one of the most comprehensive sources of altmetrics available. However, there's a lot that still isn't known about the underlying population from which the metrics are derived, and the various behaviors that they engage in as they build their Mendeley library. We are collecting the largest sample of Mendeley reader behavior and will make the survey data available to the altmetrics community. In this talk, we will describe the data collection, as well as present some of the highlights of the data so far.</p>
  </li>
  </ul>
  </li>
  <li>
  <p><strong>David Walters, Brunel University</strong></p>

  <ul>
  <li>
  <p><em>Institutional services and altmetrics as drivers for a cultural transition to open scholarship. Two souls with but a single thought!</em></p>

  <p>The causal effect to the impact of scholarly outputs disseminated under an open model may be mirrored in the statistical analysis provided by Altmetrics. In the wake of technological developments and funder expectations, we at Brunel University London have a longstanding commitment for open access to our research outputs, going back over ten years. A single campus, community focused institution, our services and systems been tailored to support our scholars and effect cultural change during the transition toward open scholarship.</p>

  <p>I will talk about how the evolution of our systems deployment has led to a support network that facilitates University publishing for new, open forms of scholarly output and that enables the monitoring of traditional published outputs through green, gold or paywall distribution models. Our publishing systems include an Institutional Repository (IR) and FigShare Data Repository. Our Current Research Information System (CRIS) provides the mechanism to monitor publication trends across our entire portfolio.</p>

  <p>With the monitoring of open academic activity fully supported by the CRIS along with partner services Cottage Labs, DOAJ, Sherpa and Core, I will outline how we developed a small centralised service around these tools tailored to foster engagement and to transform dissemination practice across our community.</p>

  <p>Two hearts that beat as one!</p>

  <p>Alongside the proliferation of social tools for researchers has been the growth of alternative metrics. From our services at ground level we are well positioned to comment on the divide that exists between those researchers who actively use their social media networks to promote the discovery of their output and those who don’t.</p>

  <p>I will discuss our ‘Altmetric for Institutions’ setup, which monitors the records held in our CRIS. I will demonstrate how this information is shared with authors, research managers and marketing to benefit different areas of institution, but in particular how this provides a powerful visual prompt to users of our service who may be unsure about the academic return of the open movements in real terms.</p>

  <p>We are beginning to see how the data we work with every day could be used to extend the discovery of our academics work and to promote the institutions reputation in this space. We curate a huge range of high quality metadata within our CRIS; keywords, subjects and themes to name but a few. We want to see better ways of using this data to select and promote our publications across the social sphere – ideally making use of and developing our existing local networks and the networks of our researchers, and I will speak to our progress in this area so far.</p>

  <p>I will conclude by arguing that the clear, mutualistic relationship between the altmetrics and open science movements necessitates effective co-operation with local university services to bring about a smooth and swift transition for authors to the open scholarly model.</p>
  </li>
  </ul>
  </li>
  <li>
  <p><strong>Kim Holmberg, University of Turku, Finland</strong></p>

  <ul>
  <li>
  <p><em>Measuring the societal impact of open science</em></p>

  <p>Altmetrics, which refers to both the research field and the altmetric events being investigated, has the potential to provide a more nuanced understanding of where and how research has had an impact. For instance, while citations only reflect scientific impact, tweets, blog citations, Facebook likes, and so on may be able to reflect other types of impact such as societal impact on various audiences. Altmetrics is also closely connected to the Open Science movement, as both have emerged from the transition of scholarly communication to the web. In addition, while the open science movement still lacks incentives for individual researchers to adopt some of the ideologies of the movement, which in turn hinders the rapid assimilation of it, altmetrics provides novel indicators for attention, visibility, and impact that could stimulate the interest of researchers. As researchers see how their work is gaining attention online, it might motivate them to share more of their work openly.</p>

  <p>Our study will present preliminary results from a new research project (financed by the Ministry of Education in Finland) that works at the intersection of altmetrics and open science. The project investigates the connection between altmetrics and open science, maps the current state of research in Finland using altmetric research methods and data, and develops new indicators to measure societal impact and new methods to study these phenomena.</p>

  <p>Our preliminary results on the connection between altmetrics and open access publications show a significant difference between altmetric indicators based on number of mentions of open access publications in their content. Open access articles receive more attention on Facebook and (especially) on Twitter compared to articles that are behind paywalls. The opposite appears to be the case for Mendeley readers and (to some extent) for Wikipedia posts, as open access articles are not used as much as those that are behind paywalls in these contexts. The preliminary results are based on an analysis of how almost 4 million altmetric events are spread between articles in open access journals (as listed by DOAJ) and other journals.</p>

  <p>The results support the assumption that Twitter especially might be able to reflect the attention of a wider public, while Mendeley is used mainly by researchers. In addition, the preliminary results also suggest that a great deal of the Wikipedia articles are in fact written by researchers with paid access to research articles, providing additional evidence of the high quality of Wikipedia articles.</p>
  </li>
  </ul>
  </li>
  </ul>

  <hr>

  <h4>12:05 – 13:05:<br>Lunch (&amp; Posters)</h4>

  <hr>

  <h4 id="research-evaluation">13:05 - 13:50:<br>Altmetrics &amp; Research Evaluation</h4>
  <p><strong>Chair:</strong> Erik van Aert, NWO </p>
  <ul>
  <li>
  <p><strong>Thed van Leeuwen, Rodrigo Costas, &amp; Clifford Tatum (CWTS – Leiden University)</strong></p>

  <ul>
  <li>
  <p><em>Slowing the pace on applying metric techniques on Open Science</em></p>

  <p>Traditionally, advanced bibliometrics have been the ‘gold standard’ in research evaluations in many fields. Due to changes in communication patterns in various fields, we now see alternative ways of assessing research appearing on the landscape. One of the major developments in scientific communication patterns is the advent of the Openness movement, through which various activities in academic life become more democratic, transparent, and hopefully fairer. This stretches out to publishing and the costs involved, how data are shared, and how peer review is organized, to name some instances in which the issue of Openness is raised. Of a somewhat more recent nature is the way assessment of scholarly activity is organized, in particular with respect to the way the various audiences with whom scholars are communicating are considered. A new way of looking at research assessment is through the recent ‘alternative metrics’ or also referred to as Altmetrics.</p>

<p>More classical bibliometrics are under pressure, due to international (DORA-Declaration) and national debates and initiatives (SiT) related to the organization of research assessment in various layers of the science system. This stirs a re-focus from science policy towards alternative ways to assess research performance. In this presentation we will show, by a recent example, how careful we have to be in making choices for metrics in order to support research assessment practices as well as science policy decision making.</p>
</li>
  </ul>
  </li>
  <li>
  <p><strong>Peter van den Besselaar (VU University Amsterdam)</strong></p>

  <ul>
  <li>
  <p><em>What is altmetrics and how to proceed with it?</em></p>

  <p>Bibliometrics was driven by availability of data: Web of Science, and the data enabled but also limited the kind of indicators for scholarly performance. Most indicators are variants of publication and citation counts, and have as we know many limitations. Altmetrics follows to some extent a similar development. New data are deployed to develop new indicators. However, I will argue that one needs to understand the dynamics of the science system in order to develop adequate indicators.</p>

<p>In this presentation I will first discuss the limits of publication and citation counts, as well as of peer review. Then I discus a few examples of Altmetrics: Indicator for societal impact of research-infrastructures, indicators for independence of researchers, and indicators for the performance of research groups. What can we learn from these examples? The issue is not to replace not so good indicators by peer review as is increasingly argued, but to develop theory based smart Altmetrics to support selection and decision-making. The examples may show how that can work.</p>
</li>
  </ul>
  </li>
  <li>
  <p><strong>Eppo Bruins &amp; Rens Vandeberg (STW / NWO)</strong></p>

  <ul>
  <li>
  <p><em>Innovation indicators: Why the future is so hard to predict</em></p>

  <p>In science and innovation, quality indicators are usually based on counting uniform and globally applicable parameters. Whereas this completely ignores the pluriform character of reality, any differentiated and more nuanced approach has usually led to a zoo of deliverables, hampering comparison and serious evaluation of actions. We present an impact-driven approach, the 4D-model for valorisation, which overcomes these shortcomings and which helps to get the 'real talk on the table'. We also present the first application of this approach for the evaluation of innovation programs and funding instruments.</p>
 </li>
  </ul>
  </li>
  </ul>

  <hr>

  <h4 id="beyond-the-article">13:50 - 15:05:<br>Beyond the article: tracking other research outputs</h4>

  <p><strong>Chair:</strong> TBC</p>

  <ul>
  <li>
  <p><strong>Josh Borrow (Durham University) &amp; Pedro Russo (Leiden Observatory)</strong></p>

  <ul>
  <li>
  <p><em>A Blueprint for Assessing Societal Impact Through Public Engagement</em></p>

  <p>Societal impact of research outputs are difficult to measure. Over the past few decades, there has been a huge increase in the amount of public engagement performed by researchers. These initiatives are generally poorly assessed, with abstract searches for learning impact rather than focusing on development of technique. The assessments that are being implemented tend to be motivated by funding bodies' quest for 'societal impact' and require a focus on abstract metrics that are generally irrelevant and difficult to measure.</p>

  <p>In this presentation, we describe a new method for assessing public engagement events in research. This evaluation framework, as well as supporting researchers in public engagement, will lead to meaningful appraisals of initiatives such that researchers can receive career development for their work. All of this will lead to a greater respect for public engagement within research institutions, fighting the anti-engagement atmosphere and promoting young researchers to engage with the public and have a impact on society.</p>
  </li>
  </ul>
  </li>
  <li>
  <p><strong>Robin Haunschild, Max Planck Institute for Solid State Research</strong></p>

  <ul>
  <li>
  <p><em>Networks of reader and country status: An analysis of Mendeley reader statistics</em></p>

  <p>Mendeley is one of the most important sources for altmetrics (Bornmann, 2014). Here, bookmarks of papers in this popular online reference manager are counted which can be expected to reflect readership of publications. Since Mendeley does not only provide straight reader counts, but also itemized counts for different reader groups, the data can be used for sophisticated analyses. In order to reveal the structure of complex data, social network analysis provides the most suitable methods (de Nooy, Mrvar, &amp; Batagelj, 2011).</p>

  <p>In this study, we analyze Mendeley readership data of a set of 1,133,224 articles and 64,960 reviews with publication year 2012. In total, we found 9,352,424 reader counts for articles (8.25 reader counts per article on average) and 1,335,764 reader counts for reviews (20.56 reader counts for reviews on average). The Mendeley reader count information was gathered by requests to the Mendeley API. The academic status of the users was available in all cases, the sub-discipline information was available in nearly all cases, but the country information was available only for 49.3% of the articles and 66.1% of the reviews. In total, we observed 1,572,240 reader counts (16.8%) for articles and 212,693 reader counts (15.9%) for reviews where the location information was shared.</p>

  <p>Three respective networks were generated and groups were distinguished using the community-finding algorithm of Blondel, Guillaume, Lambiotte, and Lefebvre (2008): (1) The network based on disciplinary affiliations of Mendeley readers contains four groups: (i) biology, (ii) social science and humanities (including relevant computer science), (iii) bio-medical sciences, and (iv) natural science and engineering. In all four groups, the category with the addition “miscellaneous” prevails. (2) The network of co-readers in terms of professional status shows that a common interest in papers is mainly shared among PhD students, Master’s students, and postdocs. (3) The country network focusses on global readership patterns: a group of 53 nations is identified as core to the scientific enterprise, including most OECD countries, Russia, and China.</p>
  </li>
  </ul>
  </li>
  <li>
  <p><strong>Sünje Dallmeier-Tiessen (CERN) &amp; Merce Crosas (Harvard University)</strong></p>

  <ul>
  <li>
  <p><em>Comprehensive tracking of research workflows using Altmetrics in Social Sciences and High-Energy Physics</em></p>

  <p>Altmetrics are becoming increasingly pervasive for traditional scholarly objects. However, Open Science and scholarly communication move beyond the sharing of “only” articles and data, but also include code, models, documentations, visualizations etc.. For future reinterpretation, reuse or reproducibility of research results it is indispensable to preserve these scholarly elements to make them accessible and retrievable in the long term.</p>

  <p>The products that should be considered result from research workflows through the lifetime of (ongoing) experiments or projects and, thus, the objects might be subject to change over time while they should be referenceable for others. Using altmetrics as a tool to track and visualize the use or impact of such materials could provide an important incentive to researchers (and service providers) to engage in this new realm of preservation and publishing activities. However, it is needed to understand how to expand the altmetrics concept to apply to non traditional objects and their preservation and publishing challenges.</p>

  <p>This presentation highlights two examples from two very different communities, the Social Sciences and High-Energy Physics. At Harvard’s Institute for Quantitative Social Sciences and at CERN preservation and Open Science tools are being built and are (partially) already in use for both communities.</p>

  <p>We present a comparative study of the different communities, their research processes and how this impacts the resulting scholarly outputs. We will highlight the different needs in terms of preservation and Open Science and underline how this impacts the need, potential use and exposure of altmetrics - for data and other non traditional research outputs. Certainly, there are commonalities between the disciplines and their services, e.g. when using standards or emerging concepts to support data citation. However, when supporting the research workflows more comprehensively, one touches specific challenges like sensitive data or dynamic data which both demand specific solutions. In the presentation we will investigate how this (might) impacts usage and statistics (or not).</p>
  </li>
  </ul>
  </li>
  <li><p><strong>Martin Fenner, DataCite</strong></p></li>
  <li><strong>Martijn Roelaandse, Springer</strong></li>
  </ul>

  <hr>

  <h4>15:05 - 15:25:<br>Coffee break</h4>

  <hr>

  <h4 id="manifesto">15:25 - 16:40:<br>Altmetrics Manifesto: Five years on</h4>

  <p><strong>Chair:</strong> Jennifer Lin, CrossRef</p>

  <ul>
  <li><strong>Cameron Neylon, Curtin University</strong></li>
  <li><strong>Dario Taraborelli, Wikimedia Foundation</strong></li>
  <li><strong>Jason Priem, Impact Story</strong></li>
  <li><strong>Paul Groth, Elsevier Labs</strong></li>
  </ul>

  <hr>

  <h4 id="wrap-up">16:15 - 16:30:<br>Conference wrap-up</h4>
</div>
{% include footer.html %}
